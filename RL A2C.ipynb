{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fd8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from a2c_ppo_acktr import algo, utils\n",
    "from a2c_ppo_acktr.algo import gail\n",
    "from a2c_ppo_acktr.arguments import get_args\n",
    "from a2c_ppo_acktr.envs import make_vec_envs\n",
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from evaluation import evaluate\n",
    "\n",
    "import gym_pendrogone\n",
    "from utils import PlanarQuadrotorDynamicsWithInvertedPendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012f59c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.algo = 'a2c'\n",
    "        self.lr = 7e-4\n",
    "        self.eps = 1e-5\n",
    "        self.alpha = 0.99\n",
    "        self.gamma = 0.99\n",
    "        self.use_gae = False\n",
    "        self.gae_lambda = 0.95\n",
    "        self.entropy_coef = 0.01\n",
    "        self.value_loss_coef = 0.5\n",
    "        self.max_grad_norm = 0.5\n",
    "        self.seed = 1\n",
    "        self.num_processes = 1\n",
    "        self.num_steps = 5\n",
    "        self.log_interval = 10\n",
    "        self.save_interval = 100\n",
    "        self.eval_interval = None # Not supported because not using wrapper env structs\n",
    "        self.num_env_steps = 1e6\n",
    "        self.env_name = 'Pendrogone-v0'\n",
    "        self.log_dir = '/tmp/gym/'\n",
    "        self.save_dir = './trained_models/'\n",
    "        self.no_cuda = False\n",
    "        self.use_proper_time_limits = False\n",
    "        self.recurrent_policy = False\n",
    "        self.use_linear_lr_decay = False\n",
    "        \n",
    "        self.cuda = not self.no_cuda and torch.cuda.is_available()\n",
    "        \n",
    "        assert self.algo in ['a2c', 'ppo', 'acktr']\n",
    "        if self.recurrent_policy:\n",
    "            assert self.algo in ['a2c', 'ppo'], \\\n",
    "                'Recurrent policy is not implemented for ACKTR'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if args.cuda and torch.cuda.is_available() and args.cuda_deterministic:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "log_dir = os.path.expanduser(args.log_dir)\n",
    "eval_log_dir = log_dir + \"_eval\"\n",
    "utils.cleanup_log_dir(log_dir)\n",
    "utils.cleanup_log_dir(eval_log_dir)\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7ad02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinlee/anaconda3/lib/python3.7/site-packages/gym/spaces/box.py:74: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  \"Box bound precision lowered by casting to {}\".format(self.dtype)\n"
     ]
    }
   ],
   "source": [
    "# envs = make_vec_envs(args.env_name, args.seed, args.num_processes, args.gamma, args.log_dir, device, False)\n",
    "\n",
    "# Not using wrapper envs allow code to run without errors, but can't use provided evaluation code\n",
    "env = gym.make('Pendrogone-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e35e981b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (base): MLPBase(\n",
       "    (actor): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (critic): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (critic_linear): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (dist): DiagGaussian(\n",
       "    (fc_mean): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (logstd): AddBias()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_critic = Policy(\n",
    "    env.observation_space.shape,\n",
    "    env.action_space,\n",
    "    base_kwargs={'recurrent': args.recurrent_policy})\n",
    "actor_critic.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd5ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = algo.A2C_ACKTR(\n",
    "    actor_critic,\n",
    "    args.value_loss_coef,\n",
    "    args.entropy_coef,\n",
    "    lr=args.lr,\n",
    "    eps=args.eps,\n",
    "    alpha=args.alpha,\n",
    "    max_grad_norm=args.max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e85f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = RolloutStorage(args.num_steps, args.num_processes,\n",
    "                              env.observation_space.shape, env.action_space,\n",
    "                              actor_critic.recurrent_hidden_state_size)\n",
    "\n",
    "obs = env.reset()\n",
    "obs = torch.from_numpy(obs).float().to(device)\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=10)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a903f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FPS 417: mean/median reward -50210.7/-50216.0, min/max reward -50245.9/-50170.6:  22%|██▏       | 43681/200000 [08:43<31:14, 83.41it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-250246a1f4ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m                                  args.gae_lambda, args.use_proper_time_limits)\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Data/Stanford/AA203/project/AA203_Final_Project/a2c_ppo_acktr/algo/a2c_acktr.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         (value_loss * self.value_loss_coef + action_loss -\n\u001b[0;32m---> 72\u001b[0;31m          dist_entropy * self.entropy_coef).backward()\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macktr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "num_updates = int(args.num_env_steps) // args.num_steps // args.num_processes\n",
    "with tqdm(range(num_updates)) as pbar:\n",
    "    for j in pbar:\n",
    "\n",
    "        if args.use_linear_lr_decay:\n",
    "            # decrease learning rate linearly\n",
    "            utils.update_linear_schedule(\n",
    "                agent.optimizer, j, num_updates,\n",
    "                agent.optimizer.lr if args.algo == \"acktr\" else args.lr)\n",
    "\n",
    "        for step in range(args.num_steps):\n",
    "            # Sample actions\n",
    "            with torch.no_grad():\n",
    "                value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "            # Observe reward and next obs\n",
    "            action = action[0]\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            reward, done, infos = np.array([reward]), [done], [info]\n",
    "            obs = torch.from_numpy(obs).float().to(device)\n",
    "            reward = torch.from_numpy(reward).unsqueeze(dim=1).float()\n",
    "\n",
    "#             for info in infos:\n",
    "#                 if 'episode' in info.keys():\n",
    "#                     episode_rewards.append(info['episode']['r'])\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor(\n",
    "                [[0.0] if done_ else [1.0] for done_ in done])\n",
    "            bad_masks = torch.FloatTensor(\n",
    "                [[0.0] if 'bad_transition' in info.keys() else [1.0]\n",
    "                 for info in infos])\n",
    "            rollouts.insert(obs, recurrent_hidden_states, action,\n",
    "                            action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_value = actor_critic.get_value(\n",
    "                rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "                rollouts.masks[-1]).detach()\n",
    "\n",
    "        rollouts.compute_returns(next_value, args.use_gae, args.gamma,\n",
    "                                 args.gae_lambda, args.use_proper_time_limits)\n",
    "\n",
    "        value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "        losses.append((value_loss, action_loss, dist_entropy))\n",
    "\n",
    "        rollouts.after_update()\n",
    "\n",
    "        # save for every interval-th episode or for the last epoch\n",
    "        if (j % args.save_interval == 0\n",
    "                or j == num_updates - 1) and args.save_dir != \"\":\n",
    "            save_path = os.path.join(args.save_dir, args.algo)\n",
    "            try:\n",
    "                os.makedirs(save_path)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "            torch.save([\n",
    "                actor_critic,\n",
    "                getattr(utils.get_vec_normalize(env), 'obs_rms', None)\n",
    "            ], os.path.join(save_path, args.env_name + \".pt\"))\n",
    "\n",
    "        if j % args.log_interval == 0 and len(episode_rewards) > 1:\n",
    "            total_num_steps = (j + 1) * args.num_processes * args.num_steps\n",
    "            end = time.time()\n",
    "            pbar.set_description(\"FPS {}: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\"\n",
    "                .format(int(total_num_steps / (end - start)),\n",
    "                        np.mean(episode_rewards), np.median(episode_rewards),\n",
    "                        np.min(episode_rewards), np.max(episode_rewards)))\n",
    "#             print(\n",
    "#                 \"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"\n",
    "#                 .format(j, total_num_steps,\n",
    "#                         int(total_num_steps / (end - start)),\n",
    "#                         len(episode_rewards), np.mean(episode_rewards),\n",
    "#                         np.median(episode_rewards), np.min(episode_rewards),\n",
    "#                         np.max(episode_rewards), dist_entropy, value_loss,\n",
    "#                         action_loss))\n",
    "\n",
    "        if (args.eval_interval is not None and len(episode_rewards) > 1\n",
    "                and j % args.eval_interval == 0):\n",
    "            obs_rms = utils.get_vec_normalize(env).obs_rms\n",
    "            evaluate(actor_critic, obs_rms, args.env_name, args.seed,\n",
    "                     args.num_processes, eval_log_dir, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "_, ax = plt.subplots(1,3, figsize=(18,6))\n",
    "ax[0].plot(losses[:,0], label=\"value_loss\")\n",
    "ax[1].plot(losses[:,1], label=\"action_loss\")\n",
    "ax[2].plot(losses[:,2], label=\"dist_entropy\")\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b44392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(actor_critic, obs_rms, env_name, seed, num_processes, eval_log_dir,\n",
    "             device):\n",
    "    # eval_envs = make_vec_envs(env_name, seed + num_processes, num_processes,\n",
    "    #                               None, eval_log_dir, device, True)\n",
    "\n",
    "    eval_envs = gym.make('Pendrogone-v0')\n",
    "\n",
    "    vec_norm = utils.get_vec_normalize(eval_envs)\n",
    "    if vec_norm is not None:\n",
    "        vec_norm.eval()\n",
    "        vec_norm.obs_rms = obs_rms\n",
    "\n",
    "    eval_episode_rewards = []\n",
    "\n",
    "    obs = eval_envs.reset()\n",
    "    eval_recurrent_hidden_states = torch.zeros(\n",
    "        num_processes, actor_critic.recurrent_hidden_state_size, device=device)\n",
    "    eval_masks = torch.zeros(num_processes, 1, device=device)\n",
    "\n",
    "    while len(eval_episode_rewards) < 10:\n",
    "        with torch.no_grad():\n",
    "            _, action, _, eval_recurrent_hidden_states = actor_critic.act(\n",
    "                obs,\n",
    "                eval_recurrent_hidden_states,\n",
    "                eval_masks,\n",
    "                deterministic=True)\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, _, done, infos = eval_envs.step(action)\n",
    "\n",
    "        eval_masks = torch.tensor(\n",
    "            [[0.0] if done_ else [1.0] for done_ in done],\n",
    "            dtype=torch.float32,\n",
    "            device=device)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                eval_episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "    eval_envs.close()\n",
    "\n",
    "    print(\" Evaluation using {} episodes: mean reward {:.5f}\\n\".format(\n",
    "        len(eval_episode_rewards), np.mean(eval_episode_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf88c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_rms = utils.get_vec_normalize(env).obs_rms\n",
    "evaluate(actor_critic, obs_rms, args.env_name, args.seed,\n",
    "                     args.num_processes, eval_log_dir, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make('Pendrogone-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "rewards = []\n",
    "\n",
    "recurrent_hidden_states = torch.zeros(1, actor_critic.recurrent_hidden_state_size)\n",
    "masks = torch.zeros(1, 1)\n",
    "obs = eval_env.reset()\n",
    "obs = torch.from_numpy(obs).float().to(device)\n",
    "\n",
    "for _ in range(1000):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        value, action, _, recurrent_hidden_states = actor_critic.act(\n",
    "            obs, recurrent_hidden_states, masks, deterministic=True)\n",
    "    \n",
    "    # torch.Size([1, 2, 1, 2])\n",
    "    action = action[0][0][0]\n",
    "    obs, reward, done, _ = eval_env.step(action)\n",
    "    obs = torch.from_numpy(obs).float().to(device)\n",
    "    \n",
    "    masks.fill_(0.0 if done else 1.0)\n",
    "    # print(obs)\n",
    "    frames.append(eval_env.render(mode = 'rgb_array'))\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.cumsum(rewards))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=25)\n",
    "    anim = HTML(anim.to_html5_video())\n",
    "    # display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5441ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cf452",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294db103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
